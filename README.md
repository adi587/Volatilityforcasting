# GARCH

Firstly we see how volatility is defind. $u_i=\frac{S_i-S_{i-1}}{S_{i-1}}$ where $S_i$ is stock price at day $i$ and $u_i$ is the percentage change in price between day $i$ and $i-1$. We assume $\tilde{u}=\frac{1}{m}$ $\sum_{i=0}^{m} u_{n-i}=0$ were $\tilde{u}$ is average percentage change of stock price on day $n$ out of $m$ observations of $u_i$. The reason why this can be assumed to be 0 is explained in the Hull book. The volatility on day $n$ is given as $\sigma_n^2=\frac{1}{m}$ $\sum_{i=1}^m u^2_i$. The GARCH(1,1) model to predict volatility for day $n$ is $\sigma_n^2=\gamma V_L+\alpha u_{n-1}^2+\beta \sigma_{n-1}^2$ ($V_L$ is the long running average variance rate and $\gamma+\alpha+\beta=1$). This is a type of expontentially weighted moving average model (recent observations of $u_i$ matter more to current volatility estimate than later ones). $\sigma_{n-1}^2$ here is the estimated volaility the previous day (through the model). This model only requires knowledge of the previous days data (the data for days before this is all encoded in the predicted $\sigma_{n-1}^2$. The GARCH model has several useful properties. Firstly it predicts (correctly) volatility to be mean reverting to $V_L$. This can be done through showing through the GARCH model, varaince predictions (V) satisfies $dV=\gamma (V_L-V)dt+(\alpha/\sqrt{2})Vdz$ (TODO). The 'strength' of this 'pull' back to $V_L$ is determined by $\gamma$. To understand the other 2 terms we can look at the GARCH model recursively. When subsituting the $\sigma_{n-1}^2$ and $\sigma_{n-2}^2$ predictions to $\sigma_n^2$ we see: $\sigma_n^2=\gamma V_L+\gamma\beta V_L+\gamma \beta^2 V_L+\alpha u_{n-1}^2+\alpha \beta u_{n-2}^2+\alpha \beta^2u_{n-3}^2+\beta^3\sigma_{n-3}^2$. This shows that $\beta$ can be interpreted as a decay rate between the importance of subsequent observations in predicting volatility. I.E if $\beta$ is large then $u_{n-2},u_{n-3}$ play more importance to what the total volatility should be than if $\beta$ were small. To now use this model we need to find the most likely parameters of $\gamma,\alpha,\beta$ given a dataset of price movements. Setting $\omega=\gamma V_L$ gives $\sigma_n^2=\omega+\alpha u_{n-1}^2+\beta \sigma_{n-1}^2$ which is the model we will implement (we will find optimal $\omega,\alpha,\beta$).

To find the optimal parameters we first assume the underlying price distribution is lognormal. So an observation of $u_i$ should occur with likelyhood $\sim \frac{1}{\sqrt{2\pi\sigma_i^2}}\exp{\frac{-u_i^2}{2\sigma_i^2}}$ where $\sigma_i^2$ is the variance for day $i$. Given set of $n$ observations the likelyhood that the set of $u_i$'s are observed is $\Pi_{i=1}^m \frac{1}{\sqrt{2\pi\sigma_i^2}}\exp{\frac{-u_i^2}{2\sigma_i^2}}$. The best estimates of $\sigma_i$ is the one that maximises this likelyhood function. However attempting to do this on a function that multiplies the different observations is hard, instead the logarithim can be taken which (ignoring constant terms) gives $\sum_{i=1}^m -\ln (\sigma_i^2)-\frac{u_i^2}{\sigma_i^2} $. As the log function is montonically increasing, mininmising $\sum_{i=1}^m \ln (\sigma_i^2)+\frac{u_i^2}{\sigma_i^2} $ is equivalent to maximising the likelyhood function. To do this in practice an Adam optimiser is used. 

To first test how long it takes the model to converge simulation data was first generated. This is done by predetermining ($\alpha,\beta,\omega$) which were set to $0.2,0.7,0.1$ initially. Then an initial $\sigma_0$ is provided from which the first $u$ movement is sampled from using a normal distribution ($u=\sigma N(0,1)$, where $N(0,1)$ is standard normal distribution). The next $\sigma$ is then estimated through the GARCH model to sample the next $u$. This process is repeated giving multiple $u$ which simulate a price movement that follows the GARCH model. The optimiser is then used on this data to find the original parameters. A simulated price movement is shown here:
![image](https://github.com/adi587/Volatilityforcasting/assets/63116085/25548857-40dc-4534-8152-b14fd45b54ad)

The volatility can be seen to be mean reverting. Furthermore in areas of large price movements the volatility increases to simulate a high volatility period before reverting back.

I first simulated data for 500,750,1000 days 5 times each. The initial 'guess' of the parameters were $0.3,0.6,0.05$ for $\alpha,\beta,\omega$ respectively. I set the learning rate of the ADAM optimiser to 0.004 (played around with few different rates but this seemed to work best, when I do a more formal investigation I will post the results here). Here are the predicted parameters using the optimiser.


I plotted the mean and range of values for each day (used range instead of standard deviation to illustrate the wide span of values possible from even a small set of perfectly GARCH simulated set of data). As can be seen there is a a large range in the parameter values however the means do tend towards to the actual values as days sampled are increased. The range does also decrease for more days. Days longer than 1000 took too long for my computer to output a value hence I was limited in the sample space I could investigate. Note that this price movement was completely simulated to perfectly follow a GARCH volatility process. Therefore using this model on real worls data should give us worse results. Given the already large range of parameters in this idealised situation I do not think this optimiser and model will do well on real world data with the current maximum day range I can use. Assuming the volaility followed closely to GARCH model, if we used the current optimiser on the maxmimum possible days sampled (before my laptop takes too long) we are likely to be in the wide error bars rather than predicting close to actual value. We would have to use a larger sample space for our model (if correct in predicting volatility of given stock) to give accurate GARCH parameters from which volatility can be forcasted.

A possible way this optimising process could be sped up is remembering that long running volaility average is $V_L=\frac{\omega}{1-\alpha-\beta}$. Hence we could first estimate what the long running volatility is by averaging it in a given data set. The we set $\omega=V_L(1-\alpha-\beta)$. This means we only need to optimise the program for two parameters now ($\alpha$ and $\beta$). This should cut down the program running time by a lot allowing me to sample over much larger day ranges. Note however there runs a risk that the $V_L$ we calculate over this day range is not the 'true' $V_L$ (the selected days may just have really high or low volatility which isnt representive of the actual long term average volatility), but sampled over large amount of days this error should be minimal. I will try incorporate this to the code later on and see if it imporves performance 



