# GARCH

Volatility is defined following way: $u_i=\frac{S_i-S_{i-1}}{S_{i-1}}$ where $S_i$ is stock price at day $i$ and $u_i$ is the percentage change is stock between day $i$ and $i-1$. $\tilde{u}=\frac{1}{m} \sum_{i=0}^{m} u_{n-i}=0$ where $\tilde{u}$ is average percentage change of stock price on day $n$ out of $m$ observations of $u_i$. The reason why this can be assumed to be 0 is explained in the Hull book. Volatility on day $n$ is then given as $\sigma_n^2=\frac{1}{m}\sum_{i=1}^m u^2_i$. The GARCH(1,1) model to predict volatility for day $n$ is $\sigma_n^2=\gamma V_L+\alpha u_{n-1}^2+\beta \sigma_{n-1}^2$ ($V_L$ is long run average variance rate and $\gamma+\alpha+\beta=1$). This model is a type of expontentially weighted moving average model (recent observations of $u_i$ matter more to current volatility than later ones). $\sigma_{n-1}^2$ here is the estimated volaility the previous day (through the model). This model only requires knowledge of the previous days data (the data for days before this is all encoded in predicted $\sigma_{n-1}^2$. The GARCH model has a few useful properties. First it predicts volatility to be mean reverting. This can be done through showing that in the GARCH model, varaince predictions (V) satisfies $dV=\gamma (V_L-V)dt+(\alpha/\sqrt{2})Vdz$ (TODO). The 'strength' of this 'pull' back to $V_L$ is determined by $\gamma$. To understand the other 2 terms we can look at GARCH model recursively. When subsituting the $\sigma_{n-1}^2$ and $\sigma_{n-2}^2$ prediction to $\sigma_n^2$ we see: $\sigma_n^2=\gamma V_L+\gamma\beta V_L+\gamma \beta^2 V_L+\alpha u_{n-1}^2+\alpha \beta u_{n-2}^2+\alpha \beta^2u_{n-3}^2+\beta^3\sigma_{n-3}^2$. This shows $\beta$ can be interpreted as a decay rate between the importance of subsequent observations. I.E if $\beta$ is large then $u_{n-2},u_{n-3}$ play more importance to what the total volatility should be than if $\beta$ were small. To now use this model we need to find the most likely parameters of $\gamma,\alpha,\beta$ given a dataset of price movements. Setting $\omega=\gamma V_L$ gives $\sigma_n^2=\omega+\alpha u_{n-1}^2+\beta \sigma_{n-1}^2$ which is the model we will implement (finding $\omega,\alpha,\beta$).

To find the most likely parameters we first asume the underlying price distribution is lognormal. So an observation of $u_i$ should occur with likelyhood $\sim \frac{1}{\sqrt{2\pi\sigma_i^2}}\exp{\frac{-u_i^2}{2\sigma_i^2}}$ where $\sigma_i^2$ is the variance for day $i$. Given set of $n$ observations the likelyhood that the observed $u_i$'s are observed is $\Pi_{i=1}^m \frac{1}{\sqrt{2\pi\sigma_i^2}}\exp{\frac{-u_i^2}{2\sigma_i^2}}$. The best estimates of $\sigma_i$ is the one that maximises this likelyhood function. However attempting to do this on a function that multiplies the different observations is hard, instead the logarithim can be taken which (ignoring constant terms) gives $\sum_{i=1}^m -\ln (\sigma_i^2)-\frac{u_i^2}{\sigma_i^2} $. As log function is montonically increasing, mininmising $\sum_{i=1}^m \ln (\sigma_i^2)+\frac{u_i^2}{\sigma_i^2} $ is equivalent to maximising likelyhood function. To do this practice an Adam optimiser is used. The loss function is the sum shown previously. 

To first test how long it takes the model to converge simulating data was first generated. This is done by predetermining $\alpha,\beta,\omega) which were set to $0.2,0.7,0.1$ initially. Then an initial $\sigma_i$ is provided from which the $u$ movement is sampled from this normal distribution. The next $\sigma$ is then estimated through the GARCH model to to sample the next $u$ jump. This process is repeated simulating a price movement that follows the GARCH model. The optimiser is then used on this data.

