# GARCH

Volatility is defined following way: $u_i=\frac{S_i-S_{i-1}}{S_{i-1}}$ where $S_i$ is stock price at day $i$ and $u_i$ is the percentage change is stock between day $i$ and $i-1$. $\tilde{u}=\frac{1}{m}$ $\sum_{i=0}^{m} u_{n-i}=0$ where $\tilde{u}$ is average percentage change of stock price on day $n$ out of $m$ observations of $u_i$. The reason why this can be assumed to be 0 is explained in the Hull book. Volatility on day $n$ is then given as $\sigma_n^2=\frac{1}{m}$ $\sum_{i=1}^m u^2_i$. The GARCH(1,1) model to predict volatility for day $n$ is $\sigma_n^2=\gamma V_L+\alpha u_{n-1}^2+\beta \sigma_{n-1}^2$ ($V_L$ is long run average variance rate and $\gamma+\alpha+\beta=1$). This model is a type of expontentially weighted moving average model (recent observations of $u_i$ matter more to current volatility than later ones). $\sigma_{n-1}^2$ here is the estimated volaility the previous day (through the model). This model only requires knowledge of the previous days data (the data for days before this is all encoded in predicted $\sigma_{n-1}^2$. The GARCH model has a few useful properties. First it predicts volatility to be mean reverting. This can be done through showing that in the GARCH model, varaince predictions (V) satisfies $dV=\gamma (V_L-V)dt+(\alpha/\sqrt{2})Vdz$ (TODO). The 'strength' of this 'pull' back to $V_L$ is determined by $\gamma$. To understand the other 2 terms we can look at GARCH model recursively. When subsituting the $\sigma_{n-1}^2$ and $\sigma_{n-2}^2$ prediction to $\sigma_n^2$ we see: $\sigma_n^2=\gamma V_L+\gamma\beta V_L+\gamma \beta^2 V_L+\alpha u_{n-1}^2+\alpha \beta u_{n-2}^2+\alpha \beta^2u_{n-3}^2+\beta^3\sigma_{n-3}^2$. This shows $\beta$ can be interpreted as a decay rate between the importance of subsequent observations. I.E if $\beta$ is large then $u_{n-2},u_{n-3}$ play more importance to what the total volatility should be than if $\beta$ were small. To now use this model we need to find the most likely parameters of $\gamma,\alpha,\beta$ given a dataset of price movements. Setting $\omega=\gamma V_L$ gives $\sigma_n^2=\omega+\alpha u_{n-1}^2+\beta \sigma_{n-1}^2$ which is the model we will implement (finding $\omega,\alpha,\beta$).

To find the most likely parameters we first asume the underlying price distribution is lognormal. So an observation of $u_i$ should occur with likelyhood $\sim \frac{1}{\sqrt{2\pi\sigma_i^2}}\exp{\frac{-u_i^2}{2\sigma_i^2}}$ where $\sigma_i^2$ is the variance for day $i$. Given set of $n$ observations the likelyhood that the observed $u_i$'s are observed is $\Pi_{i=1}^m \frac{1}{\sqrt{2\pi\sigma_i^2}}\exp{\frac{-u_i^2}{2\sigma_i^2}}$. The best estimates of $\sigma_i$ is the one that maximises this likelyhood function. However attempting to do this on a function that multiplies the different observations is hard, instead the logarithim can be taken which (ignoring constant terms) gives $\sum_{i=1}^m -\ln (\sigma_i^2)-\frac{u_i^2}{\sigma_i^2} $. As log function is montonically increasing, mininmising $\sum_{i=1}^m \ln (\sigma_i^2)+\frac{u_i^2}{\sigma_i^2} $ is equivalent to maximising likelyhood function. To do this practice an Adam optimiser is used. The loss function is the sum shown previously. 

To first test how long it takes the model to converge simulating data was first generated. This is done by predetermining ($\alpha,\beta,\omega$) which were set to $0.2,0.7,0.1$ initially. Then an initial $\sigma_i$ is provided from which the $u$ movement is sampled from this normal distribution. The next $\sigma$ is then estimated through the GARCH model to to sample the next $u$ jump. This process is repeated simulating a price movement that follows the GARCH model. The optimiser is then used on this data. A simulated price movement is shown here:
![image](https://github.com/adi587/Volatilityforcasting/assets/63116085/25548857-40dc-4534-8152-b14fd45b54ad)

The volatility can be seen to be mean reverting. Furthermore in areas of large price movements the volatility increases to simulate a high volatility period before reverting back.

I first simulated data for 500,750,1000 days 5 times each. The initial 'guess' of the parameters were $0.3,0.6,0.05$ for $\alpha,\beta,\omega$ respectively. Here are the predicted parameters using the optimiser.


As can be seen there is a high degree of range in the parameter values however the means do tend towards to actual value as days sampled are increased. The range does also decrease for more days. Days longer than 1000 took too long for my computer to output a value hence I was limited in the sample space I could investigate. Note that this price movement was completely simulated to perfectly follow a GARCH volatility process. Therefore using this model on real worls data should give us worse results. Given the already large range of parameters in this idealised situation I do not think this optimiser and model will do well on real world data with the current maximum day range I can use. Assuming the volaility followed closely to GARCH model, if we used the current optimiser on the maxmimum possible days sampled (before my laptop takes too long) we are likely to be in the wide error bars rather than predicting close to actual value. We would have to use a larger sample space for our model (if correct in predicting volatility of given stock) to give accurate GARCH parameters from which volatility can be forcasted.

A possible way this optimising process could be sped up is remembering that long running volaility average is $V_L=\frac{\omega}{1-\alpha-\beta}$. Hence we could first estimate what the long running volatility is by averaging it in a given data set. The we set $\omega=V_L(1-\alpha-\beta)$. This means we only need to optimise the program for two parameters now ($\alpha$ and $\beta$). This should cut down the program running time by a lot allowing me to sample over much larger day ranges. Note however there runs a risk that the $V_L$ we calculate over this day range is not the 'true' $V_L$ (the selected days may just have really high or low volatility which isnt representive of the actual long term average volatility), but sampled over large amount of days this error should be minimal. I will try incorporate this 



